\documentclass[11pt]{article}
\usepackage{nodalida2017}
%\usepackage{times}
\usepackage{mathptmx}
%\usepackage{txfonts}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{url}
\usepackage{latexsym}
\usepackage{listings}
\special{papersize=210mm,297mm} 

\title{Exploring the Expressivity of Constraint Grammar}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   {\tt email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   {\tt email@domain} }

\date{\today}

\def\t#1{\texttt{#1}}
\def\maxAmb#1{$\langle \Sigma \rangle_#1$}
\def\maxAmbFSA#1{$\langle \Sigma,S \rangle_#1$}
\def\maxAmbCFG#1{$\langle \Sigma,\Sigma^{\prime} \rangle_#1$}

\begin{document}
\maketitle

\begin{abstract}
  %%% WEN: Yeah, I realise more has to go here ^^
  We believe that for any formalism which has its roots in linguistics, it is a
  natural question to ask ``how expressive is it?''
  Therefore, in this paper, we begin to address the question of the expressivity
  of CG. 
  % For those theoretically inclined, describing the expressivity of a new language
  % formalism is a reward onto itself. 
  % However, even the more practical reader could appreciate that a better
  % understanding of a formalism can lead to novel uses and implementation
  % techniques. 
  Aside from the obvious theoretical interest, we envision also practical benefits.
  For instance, we hope that the \texttt{cgexp} tool, described in later sections
  of this paper, could eventually be developed to generate human-readable CG code
  from regular expressions or a context-free grammar. 

\end{abstract}

% \section{Introduction}

% We believe that for any formalism which has its roots in linguistics, it is a
% natural question to ask ``how expressive is it?''
% Therefore, in this paper, we begin to address the question of the expressivity
% of CG. 
% %%% WEN: We are OBLIGED to make the point here---or at very least NOT too far
% %%%      into the paper---that this question has an obvious answer. There is a
% %%%      command that allows you to call an arbitrary process, so it is
% %%%      obviously Turing complete. However, what we are interested in is how
% %%%      expressive the separate COMMANDS in CG are. What can we do with just
% %%%      REMOVE---the most common CG command? What can we do if we allow
% %%%      ourselves the use of REMOVE and some other command? Etc.
% %%% WEN: I'd say it's interesting to look at the formal language aspects BECAUSE
% %%%      it's a linguistic tool and therefore comes from a tradition of formal
% %%%      language.
% % Despite its decidedly linguistic roots, we argue that CG is interesting to look
% % at also from a computational perspective.
% %%% WEN: So basically you're saying ``we decided to have an academic wank'' it
% %%%      was definitely great. ^^
% For those theoretically inclined, describing the expressivity of a new language
% formalism is a reward onto itself. 
% %%% WEN: This is, like, a super weak claim. xD
% However, even the more practical reader could appreciate that a better
% understanding of a formalism can lead to novel uses and implementation
% techniques. 
% %%% WEN: Maybe we could take some of these hopes expressed in the following
% %%%      sentence, and rephrase them as novel uses and implementation techniques
% %%%      to satisfy our more practical readers? For instance, we could hint at a
% %%%      future version of your CGexp library which could be used to
% %%%      automatically generate CG code which disambiguates based on some
% %%%      regular expressions? You may be the better person to phrase this
% %%%      though. But I'll have a go:
% For instance, we hope that the \texttt{cgexp} tool, described in later sections
% of this paper, could eventually be developed to generate human-readable CG code
% which disambiguates phrases based on regular expressions or a context-free grammar. 

% %%% Screw introductions, that's a pretty good abstract ^^

\section{Introduction and previous work}

CG~\cite{karlsson1995constraint} was born as a practical, rather than formal, 
approach to NLP. 
Since the beginning, its authors do not envision it as a tool for 
generating strings, only for analysing and disambiguating them.
%%% WEN: Well, I'm giving the sentence above purpose here:
It is for these reasons, we believe, that the question of the expressivity of CG
as a generative formalism went unasked and unanswered for so long.
%%% WEN: ^Is this actually true? Or have folks looked into this before?^

%%% WEN: Tell the world what CG is! ^^
The standard measure of formal languages is the Chomsky hierarchy~\cite{chomsky1956hierarchy}, with its four
classes of grammars and languages, in order from most expressive to least expressive:
recursively enumerable (Type 0), context-sensitive (Type 1), context-free (Type 2), and
regular (Type 3).
The notion of expressive power, ``which constructs can we express in the language'', is coupled with parsing complexity, ``how much time and memory do we need to parse sentences in the language''; more expressive power corresponds to greater parsing complexity.

Previous work covers the expressivity of single rules, such as \texttt{IF (NOT 1* Verb OR Noun)}: 
just seeing this contextual test hints that we can express a subset of 
regular languages that contains at least disjunction, complement and 
Kleene star. \newcite{tapanainen1999phd} gives an account of the expressivity of 
a single rule for 4 different constraint formalisms. In addition, parsing 
complexity can be easily defined for a given variant and implementation of CG; 
see for instance \newcite{nemeskey14}.

However, the expressivity of the whole grammar is harder to define. 
A given grammar in CG does not describe a language, but a \emph{relation} 
between an input language and an output language. In the following section, 
we introduce our approach to emulating generation, and in the rest of the 
paper, we present some preliminary results.

\section{Expressivity of a grammar}

For this paper, we will view a constraint grammar CG as generating a formal
language $\mathcal{L}$ over an alphabet $\Sigma$ as follows:
A constraint grammar CG rejects a word $w \in \Sigma^\star$ if, when we encode
the word as a sequence of cohorts, and pass it through the CG, we get back the
dedicated rejection symbol \texttt{REJECT}. A constraint grammar CG accepts a
word $w \in \Sigma^\star$ if it does not reject it.

As an example, consider the language $a*$ over $\Sigma = \{a,b\}$. This language
can be encoded as a constraint grammar as \texttt{ADDCOHORT ("<REJECT>") BEFORE >>> IF (1* B)}.

\def\wwf{\t{"<w>"}}
\def\swf{\t{"<s>"}}
%\def\alm{\t{"a" a}~~~~~~}
%\def\blm{\t{"b" b}~~~~~~}
%\begin{figure}[h]
%  \centering
%  \begin{tabular}{cl|rl}
%    \multicolumn{2}{c|}{\textbf{Input}}  &
%    \multicolumn{2}{c|}{\textbf{Output}} \\ \hline
%    \wwf  &       & \wwf  &      \\
%          & \alm  &       & \alm \\
%          & \blm  &       &      \\
%    \wwf  &       &  \wwf &      \\
%          & \alm  &       & \alm \\
%          & \blm  &       &      \\
%    \wwf  &       &  \wwf &      \\
%          & \alm  &       & \alm \\
%          & \blm  &       &      \\
%  \end{tabular}
%  \caption{Language $a*$ for input \maxAmb{3}.}
%  \label{fig:astar}
%\end{figure}




\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{chomsky}
  \caption{Lower bound on the expressivity of CG.}
  \label{fig:nocorr}
\end{figure}


\section{CG is beyond regular}

In this section, we demonstrate some non-regular languages 
that can be expressed in CG.\footnote{More specifically, a subset that consists of
the operations \t{REMOVE, SECTION, IF, OR, NOT}.}
Due to space limitations, we describe only the rough outlines.

\subsection{Example grammar: $a^nb^n$ }

We can express the language $a^nb^n$ as a CG which disambiguates
any \maxAmb{{2n}} into $\langle a \rangle_n \langle b \rangle_n$.
In addition, we allow ourselves a hidden alphabet of helper readings, 
$\Sigma^{\prime} = \{\t{even}, \t{odd}, \t{>>>}, \t{<<<}, \t{opt\_a}, \t{opt\_b}\}$. 
%In this \maxAmbCFG{{m}}, 
Every cohort starts off with full $\Sigma$ and $\Sigma^{\prime}$.
The process goes as follows:
\begin{itemize}
\item Check if the sentence has an even number of cohorts. We know the first cohort
      is odd, and the rest is handled with rules of the form \t{REMOVE even IF (NOT -1 odd)}.
      If the last cohort is odd, then discard the sentence; otherwise continue.
\item Disambiguate edges: first one is certainly $a$ and last is certainly $b$.
\item Grow $a$s and $b$s until they meet in the middle. The disambiguation is done in two 
      passes, first removing the helper reading \t{opt\_a}, and then selecting the main reading $a$.
\end{itemize}

\subsection{Example grammar: $a^nb^nc^n$}

We can extend the previous approach to write a grammar that accepts $a^nb^nc^n$.
We use three counting symbols, three \t{opt\_X} symbols, and initially we disambiguate
the two edges and the middle. The result is the same: any \maxAmb{{3n}} gets disambiguated
into $\langle a \rangle_n \langle b \rangle_n \langle c \rangle_n$.


\section{Are all regular languages in CG?}

Is CG in any particular category? It clearly covers some regular, context-free
and context-sensitive languages, but it may just have a common subset with those 
classes, as in Figure~\ref{fig:nocorr}.
Rather than enumerating individual grammars for a given class, we need a method 
that can transform all languages in that class into CG.


We present a method to transform arbitrary finite-state automata into CG.\footnote{A subset which includes \t{LINK}, \t{NEGATE} and \t{TEMPLATE}, in addition to the previous.}
Figure~\ref{fig:fsa} presents an example automaton, with $\Sigma = \{$\emph{det,adj,noun}$\}$,
for which we implement a corresponding CG as follows.

As a modification to \maxAmb{n}, we use \maxAmbFSA{n}:
$\Sigma$ stands for maximally ambiguous \emph{word cohorts}, and $S$ for 
maximally ambiguous  \emph{state cohorts}, which are inserted between 
each $\langle \Sigma \rangle$. For example, a sequence with two transitions 
would be modelled with the following sentence \maxAmbFSA{2}, with two word cohorts:

\begin{table}[h]
\begin{tabular}{lllll}
      \swf    &    \wwf      &      \swf      &     \wwf      &     \swf     \\
 ~~~~~~\t{s1} & ~~~~\t{det}  &  ~~~~\t{ s1}   &  ~~~~\t{det}  &  ~~~~\t{s1}  \\
 ~~~~~~\t{s2} & ~~~~\t{adj}  &  ~~~~\t{ s2}   &  ~~~~\t{adj}  &  ~~~~\t{s2}  \\
              & ~~~~\t{noun} &                &  ~~~~\t{noun} &  
\end{tabular}
\end{table}

The rules of the grammar disambiguate both word cohorts and state cohorts.
Thus the desired result shows both the accepted string and the path in the automaton.

\begin{table}[h]
\centering
\begin{tabular}{lllll}
      \swf    &  \wwf       &      \swf     & \wwf           & \swf \\
 ~~~~~~\t{s1} & ~~~~\t{det} &  ~~~~\t{s2}   &  ~~~~\t{noun}  &  ~~~~\t{s1} 

\end{tabular}
\end{table}



\begin{figure}[t]
  \centering
    \includegraphics[width=0.7\linewidth]{fsa.png}
  \caption{A finite-state automaton describing the regular language \t{det (adj)* noun}.}
 \label{fig:fsa}
\end{figure}


\subsection{Disambiguation process}

% We describe the process on a high level. We have a working implementation that
% produces the rules for arbitrary automata, but the following is by no means a formal proof.

Given that every transition happens between two states, and every state 
has an incoming and outgoing transition, every rule needs no more than
positions -1 and 1 in their contextual tests. 
The semantics of the rules are ``remove a POS tag, if it is \emph{not} surrounded by allowed states'',
and ``remove a state, if it is \emph{not} surrounded by allowed transitions''.
For the example automaton, the POS-rules are as follows.

\begin{verbatim}
REMOVE...
   Det  IF (NEGATE -1 S1 LINK 2 S2) ;
   Adj  IF (NEGATE -1 S2 LINK 2 S2) ;
   Noun IF (NEGATE -1 S2 LINK 2 S1) ;
\end{verbatim}

The start and end states naturally correspond to the first and last state cohort 
in the \maxAmbFSA{n}, and can be trivially disambiguated, in this case both into \t{s1}.
Once we remove a reading from either side of a cohort, some more rules 
can take action---the context ``\t{s2} on the left side and \t{s1} on the right side''
may be broken by removing either \t{s2} or \t{s1}.
As a chain reaction, the whole sentence gets eventually disambiguated.

\subsection{Disjunction}

Consider the regular language with two strings $\{ab,ba\}$. 
Given \maxAmb{2} for any $\Sigma$, this is the closest we could get in CG output:

\begin{table}[h]
\begin{tabular}{l l}

 \wwf          &  \wwf  \\
 ~~~~~~~~~~\t{a}  &  ~~~~~~~~~~\t{a}  \\
 ~~~~~~~~~~\t{b}  &  ~~~~~~~~~~\t{b}
\end{tabular}
\end{table}

As \newcite{lager_nivre01} point out, CG has no way of expressing disjunction.
Unlike its close cousin FSIG \cite{koskenniemi90}, which would represent this language
faithfully, CG substitutes uncertainty on the sentence level (``either $ab$ or $ba$'')
with uncertainty in the cohorts:
``the first character may be either $a$ or $b$, and the second character may be either $a$ or $b$''.

An alternative would be to model languages with disjunction as a set of CGs: 
one that disambiguates any input into $ab$ and other that disambiguates 
into $ba$. But this is just speculation, we have not investigated the idea further. 

% The start and end states naturally correspond to the first and last state cohort 
% in the \maxAmbFSA{n}, and can be trivially disambiguated, in this case both into \t{s1}.
% The intermediate result is shown below:

% \def\rmd#1{{\tt \color{gray} #1}}

% \begin{table}[h]
% \centering
% \begin{tabular}{lllll}
%      \swf &  \wwf   &      \swf &     \wwf &     \swf \\
% ~~~~~~\t{s1}   & ~~~~\t{det}  &  ~~~~\t{s1}   &  ~~~~\t{det}  &  ~~~~\t{s1}   \\
% ~~~~~~\rmd{s2} & ~~~~\t{adj}  &  ~~~~\t{s2}   &  ~~~~\t{adj}  &  ~~~~\rmd{s2}   \\
% ~~~~~~         & ~~~~\t{noun} &                &  ~~~~\t{noun} &  
% \end{tabular}
% \end{table}

% Once we remove a reading from either side of a cohort, some more rules may take action.
% Now that the first state is no longer a possible \t{s2}, the rules that remove \t{adj}
% and \t{noun} readings will apply to the first word.

% \begin{table}[h]
% \centering
% \begin{tabular}{lllll}
%      \swf &  \wwf     &      \swf &     \wwf &     \swf \\
% ~~~~~~\t{s1}   & ~~~~\t{det}    &  ~~~~\t{ s1}   &  ~~~~\t{det}  &  ~~~~\t{s1}   \\
% ~~~~~~\rmd{s2} & ~~~~\rmd{adj}  &  ~~~~\t{ s2}   &  ~~~~\t{adj}  &  ~~~~\rmd{s2}   \\
% ~~~~~~         & ~~~~\rmd{noun} &                &  ~~~~\t{noun} &  
% \end{tabular}
% \end{table}

% Now the second state cohort can be disambiguated: transitions from \t{det} may only lead to \t{s2}, 
% so we remove \t{s1}. After that, we disambiguate the second word cohort, leaving only \t{noun}.
% No more rules may apply, so this is the final result.

% \begin{table}[h]
% \centering
% \begin{tabular}{lllll}
%       \swf &  \wwf     &      \swf &     \wwf  &     \swf \\
%  ~~~~~~\t{s1}   & ~~~~\t{det}    &  ~~~~\rmd{s1}  &  ~~~~\rmd{det} &  ~~~~\t{s1}   \\
%  ~~~~~~\rmd{s2} & ~~~~\rmd{adj}  &  ~~~~\t{s2}    &  ~~~~\rmd{adj} &  ~~~~\rmd{s2}   \\
%  ~~~~~~         & ~~~~\rmd{noun} &                &  ~~~~\t{noun}  &  
% \end{tabular}
% \end{table}




\section{Applications}

Does such an idea provide any foreseeable practical benefits?
The grammars derived from FSAs look clumsy, and involve extra symbols.
But if it turned out that CGs can express some interesting subset of context-free 
or context-sensitive grammars, then we could derive CGs from already existing formalisms.
This could be an alternative for learning CGs from corpus.

Compared to high-level grammar formalisms, such as (insert your favourite), CG processing
is generally faster. So a CG derived in this manner could act as a preprocessing step 
for some more expensive parser---then the rules need not be human-legible. 
The rules would be derived from the grammar itself, with the sole purpose of 
making the parsing \emph{faster}, not more accurate.



% \section*{Acknowledgments}

% Do not number the acknowledgment section. Do not include this section
% when submitting your paper for review.





\bibliographystyle{acl}
\bibliography{cg}


\end{document}
