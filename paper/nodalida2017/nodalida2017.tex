\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{nodalida2017}
\usepackage{mathptmx}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{url}
\usepackage{latexsym}
\usepackage{fancyvrb}
\special{papersize=210mm,297mm} 


\title{Exploring the Expressivity of Constraint Grammar}

\author{%
  Wen Kokke \\
  University of Edinburgh \\
  {\tt wen.kokke@ed.ac.uk} \\\And
  Inari Listenmaa \\
  University of Gothenburg \\
  {\tt inari.listenmaa@cse.gu.se} }

\date{\today}

\def\t#1{\texttt{#1}}
\def\h#1{{\tt \color{gray} #1}}
\def\swf{\h{"<s>"}}
\def\maxAmb#1{$\langle \Sigma \rangle_#1$}
\def\maxAmbFSA#1{$\langle \Sigma,S \rangle_#1$}
\def\maxAmbCFG#1{$\langle \Sigma,\Sigma^{\prime} \rangle_#1$}

\begin{document}
\maketitle

\begin{abstract}
  We believe that for any formalism which has its roots in linguistics, it is a
  natural question to ask ``how expressive is it?'' Therefore, in this paper, we
  begin to address the question of the expressivity of CG.
  Aside from the obvious theoretical interest, we envision also practical
  benefits. For instance, we hope that the FSA$\rightarrow$CG conversion tool, described in
  later sections of this paper, could eventually be developed to generate
  human-readable CG code from regular expressions or a context-free grammar. 
\end{abstract}


\section{Introduction}
For any formalism with its root in linguistics, it is natural to ask questions
such as ``How expressive is it?'' or ``Where does it sit in the Chomsky
hierarchy?''~\cite{chomsky1956hierarchy}
In this paper, we begin addressing some of these questions for constraint
grammar~\cite[CG]{karlsson1995constraint}.

Before we can even consider such a question, there is a problem we must
solve. CG was never meant to be a grammar in the generative sense. Instead, it
is a tool for analysing and disambiguating strings.
This, we believe, explains why the question of the expressivity of CG went
unasked and unanswered for a long time.
It also gives us our first problem: How do we view CGs generatively?
We address this in section~\ref{sec:gencg}.


\section{Generative Constraint Grammar}\label{sec:gencg}
We view a constraint grammar CG as generating a formal language $\mathcal{L}$
over an alphabet $\Sigma$ as follows.
We encode words $w \in \Sigma^\star$ as a sequence of cohorts, each of which has
one of the symbols of $w$ as a reading.
A constraint grammar CG rejects a word if, when we pass its encoding through the
CG, we get back the cohort \t{"<REJECT>"}. A constraint grammar CG accepts a word
if it does not reject it.
We generate the language $\mathcal{L}$ by passing every $w \in \Sigma^\star$
through the CG, and keeping those which are accepted.

As an example, consider the language $a^\star$ over $\Sigma = \{a,b\}$.
This language is encoded by the following constraint grammar:
\begin{center}
  \begin{Verbatim}
    LIST A = "a";
    LIST B = "b";
    SET LETTER = A OR B;
    SELECT A;
    ADDCOHORT ("<REJECT>")
        BEFORE LETTER 
        IF (-1 (>>>) LINK 1* B);
    REMCOHORT LETTER
        IF (-1* ("<REJECT>"));
  \end{Verbatim}
\end{center}
We then encode the input words as a series of letter cohorts with readings
(e.g.\ \(\t{"<l>"}\;\t{"a"}\), \(\t{"<l>"}\;\t{"b"}\)), and run the grammar.
For instance, if we wished to know whether either word in $\{aaa,aab\}$ is part
of the language $a^\star$, we would run the following queries:
\begin{center}
  \begin{tabular}{l|c}
    \textbf{Input}         & \textbf{Output} \\ \hline
    \(\t{"<l>"}\;\t{"a"}\) & \(\t{"<l>"}\;\t{"a"}\) \\
    \(\t{"<l>"}\;\t{"a"}\) & \(\t{"<l>"}\;\t{"a"}\) \\
    \(\t{"<l>"}\;\t{"a"}\) & \(\t{"<l>"}\;\t{"a"}\) \\ \hline
    \(\t{"<l>"}\;\t{"a"}\) & \t{"<REJECT>"} \\
    \(\t{"<l>"}\;\t{"a"}\) \\
    \(\t{"<l>"}\;\t{"b"}\)
  \end{tabular}
\end{center}
As CG is a tool meant for disambiguation, we can leverage its power to run both
queries at once:
\begin{center}
  \begin{tabular}{l|c}
    \textbf{Input}                  & \textbf{Output} \\ \hline
    \(\t{"<l>"}\;\t{"a"}\)          & \(\t{"<l>"}\;\t{"a"}\) \\
    \(\t{"<l>"}\;\t{"a"}\)          & \(\t{"<l>"}\;\t{"a"}\) \\
    \(\t{"<l>"}\;\t{"a"}\;\t{"b"}\) & \(\t{"<l>"}\;\t{"a"}\)
  \end{tabular}
\end{center}
This is a powerful feature, because it allows us disambiguate based on some
formal language $\mathcal{L}$ if we can find the CG which generates it.
However, the limitations of this style become apparent when we look at a run of
a CG for the language $\{ab,ba\}$:
\begin{center}
  \begin{tabular}{l|c}
    \textbf{Input}                  & \textbf{Output} \\ \hline
    \(\t{"<l>"}\;\t{"a"}\;\t{"b"}\) & \(\t{"<l>"}\;\t{"a"}\;\t{"b"}\) \\
    \(\t{"<l>"}\;\t{"a"}\;\t{"b"}\) & \(\t{"<l>"}\;\t{"a"}\;\t{"b"}\) \\
  \end{tabular}
\end{center}
While the output contains the interpretations $ab$ and $ba$, it also includes
$aa$ and $bb$. Therefore, while this style is useful for disambiguating using
CGs based on formal languages, it is too limited to be used in defining the
language which a CG generates.
% Wen:
%   We should include a reference to the other formalism here, or to the
%   discussion section in which we talk about the disjunction problem.
% Inari:
%   Yeah, I am wondering myself too what's the best place to talk about it.
%   Maybe it becomes easier when we have more pages to fill? 

In light of the idea of using CGs based on formal languages for disambiguating,
it seems at odds with the philosophy of CG to reject by replacing the entire
input with a single \t{"<REJECT>"} cohort. CG generally refuses to remove the
last possible reading of a cohort, under the philosophy that \emph{some}
information is certainly better than none.
However, for the definition of CG as a formal language, we need some sort of
distinctive output for rejections. Hence, we arrive at \emph{two} distinct ways
to run generative CGs: the method in which we input unambiguous strings, and
output \t{"<REJECT>"}, which is used in the definition of CG as a formal
language; and the method in which we input ambiguous strings, and simply
disambiguate as far as possible. 

It should be noted that VISL CG-3 \cite{bick2015,vislcg3} supports commands 
such as \t{EXTERNAL}, which runs an external executable. It should therefore 
be obvious that the complete set of VISL CG-3 commands, at least theoretically,
can generate any recursively enumerable language. For this reason, we will
investigate particular subsets of the commands permitted by CG.
In sections~\ref{sec:lowerbound} and~\ref{sec:regular}, we will restrict
ourselves to the subset of CG which only uses the \t{REMOVE} command with
sections, and show this to at least cover all regular languages and some
context-free and context-sensitive languages.
In section~\ref{sec:turingcomplete}, we will restrict ourselves to thesubset of
CG which only uses the \t{ADDCOHORT} and \t{REMCOHORT} commands with sections,
and show this to be Turing complete.

\section{A lower bound for CG}\label{sec:lowerbound}
In this section, we will only use the \t{REMOVE} command with sections, in
addition to a single use of the \t{ADDCOHORT} command to add the special cohort
\t{"<REJECT>"}, and a single use of the \t{REMCOHORT} command to clean up
afterwards. 
We show that, using only these commands, CG is capable of generating some
context-free and context-sensitive languages, which establishes a lower bound on 
the expressivity of CG (see Figure~\ref{fig:nocorr}).
%
\begin{figure}[h]
  \centering
  % Wen: This graphic needs to be updated, because you literally prove in the
  % next section that we cover all regular languages.
  \includegraphics[width=0.8\linewidth]{chomsky}
  \caption{Lower bound on the expressivity of the subset of CG using only \t{REMOVE}.}
  \label{fig:nocorr}
\end{figure}


\subsection{Example grammar: $a^nb^n$}
Below, we briefly describe the CG which generates the language $a^nb^n$.
This CG is defined over the alphabet $\Sigma$, in addition to a hidden alphabet
$\Sigma^\prime$. These hidden symbols are meant to serve as a simple form of
memory. When we encode our input words, we tag each cohort with \emph{every}
symbol in the hidden alphabet\footnote{
  We can automatically add these hidden symbols to our cohorts using a single
  application of the \t{ADD} command.
}, e.g.\ for some symbol $\ell \in \Sigma$ and $\Sigma' = \{h_1,\dots,h_n\}$ we
would create the cohort \(\t{"<\(\ell\)>"}\;\t{"h\(_1\)"}\;\dots\;\t{"h\(_n\)"}\).

The CG for $a^nb^n$ uses the hidden alphabet \{\t{odd}, \t{even}, \t{opt\_a},
\t{opt\_b}\}. These symbols mean that the cohort they are attached to is in an
even or odd position, and that $a$ or $b$ is a legal option for this cohort,
respectively. The CG operates as follows: 
\begin{enumerate}
\item
  Is the number of characters even? We know the first cohort is odd, and the
  rest is handled with rules of the form \t{REMOVE even IF (NOT -1 odd)}. If the
  last cohort is odd, then discard the sentence. Otherwise continue\dots
\item
  The first cohort is certainly $a$ and last is certainly $b$, so we can
  disambiguate the edges: 
  \t{REMOVE opt\_b IF (NOT -1 (*))}, and \t{REMOVE opt\_a IF (NOT 1 (*))}. 
\item
  Disambiguate the second cohort as $a$ and second-to-last as $b$, the third as
  $a$ and third-to-last as $b$, etc, until the two ends meet in the middle. If
  every \t{"<a>"} is marked with \t{opt\_a}, and every \t{"<b>"} with
  \t{opt\_b}, we accept. Otherwise, we reject.  
\end{enumerate}
The language $a^nb^n$ is context-free, and therefore CG must at least partly
overlap with the context-free languages.


\subsection{Example grammar: $a^nb^nc^n$}
We can extend the approach used in the previous grammar to write a grammar which
accepts $a^nb^nc^n$. Essentially, we can adapt the above grammar to find the
middle of any input string. Once we have the middle, we can ``grow'' $a$s from
the top and $b$s up from the middle, and $b$s down from the middle and $c$s up
from the bottom, until we divide the input into three even chunks.
If this ends with all \t{"<a>"}s marked with \t{opt\_a}, all \t{"<b>"}s marked
with \t{opt\_b}, and all \t{"<c>"}s marked with \t{opt\_c}, we accept.
Otherwise, we reject.

The language $a^nb^nc^n$ is context-sensitive, and therefore CG must at least
partly overlap with the context-sensitive languages. 
%
\begin{figure}[t]
  \centering
  \includegraphics[width=0.6\linewidth]{fsa.png}
  \caption{A finite-state automaton describing the regular language \t{det
      (adj)* n}.}
 \label{fig:fsa}
\end{figure}


\section{Are all regular languages in CG?}\label{sec:regular}
In the present section, we propose a method to transform arbitrary finite-state
automata into CG. Figure~\ref{fig:fsa} presents an example automaton with
$\Sigma = \{$\emph{det, adj, n}$\}$, which checks whether some assignment of
part-of-speech tags is ``valid''. We implement a corresponding CG in the
following sections.

\subsection{Cohorts and sentences}
As previously, we define a hidden alphabet $\Sigma^{\prime} = \{$\t{opt\_det,
  opt\_adj, opt\_n}$\}$, and insert the full set $\Sigma^{\prime}$ into each
cohort as readings. In addition, we introduce \emph{state cohorts}, which
contain the full set $S = \{s_1, s_2\}$. For example, the sequence \emph{det n}
would be modelled with the following sentence:
%
\begin{center}
  \renewcommand{\tabcolsep}{2.5pt}
  \begin{tabular}{ccccc}
    \swf   & \t{"<the>"}  & \swf   & \t{"<book>"} & \swf   \\ 
    \h{s1} & \t{det}      & \h{s1} & \t{n}        & \h{s1} \\
    \h{s2} & \t{opt\_det} & \h{s2} & \t{opt\_det} & \h{s2} \\
           & \t{opt\_adj} &        & \t{opt\_adj} &        \\
           & \t{opt\_n}   &        & \t{opt\_n}   &        
  \end{tabular}
\end{center}
%
The rules of the grammar disambiguate both word cohorts and state cohorts. Thus
the desired result shows both the accepted string and the path in the automaton,
as follows:
%
\begin{center}
  \renewcommand{\tabcolsep}{2.5pt}
  \begin{tabular}{ccccc}
    \swf   & \t{"<the>"}  & \swf   & \t{"<book>"} & \swf   \\ 
    \h{s1} & \t{det}      & \h{s2} & \t{n}        & \h{s1} \\
           & \t{opt\_det} &        & \t{opt\_n}   &        
  \end{tabular}
\end{center}


\subsection{Rules}
Given that every transition happens between two states, and every state 
has an incoming and outgoing transition, every rule needs only
positions -1 and 1 in its contextual tests. 
The semantics of the rules are ``remove an \t{opt\_POS} tag, if it is 
\emph{not} surrounded by allowed states'',
and ``remove a state, if it is \emph{not} surrounded by allowed transitions''.
For the example automaton, the \t{opt\_POS}-rules are as follows:
\begin{verbatim}
REMOVE...
  opt_det IF (NEGATE -1 S1 LINK 2 S2);
  opt_adj IF (NEGATE -1 S2 LINK 2 S2);
  opt_n   IF (NEGATE -1 S2 LINK 2 S1);
\end{verbatim}
The start and end states naturally correspond to the first and last
state cohort, and can be trivially disambiguated, in this case both into \t{s1}.
Once we remove a reading from either side of a cohort, some more rules can take
action---the context ``\t{s2} on the left side and \t{s1} on the right side''
may be broken by removing either \t{s2} or \t{s1}. 

If there is only one allowed string of length $n$ in the language, then the
result should have only one \t{opt\_POS} reading per cohort. 
% If the original cohorts are unambiguous before adding the hidden readings,  we
% use the opt-readings to accept the string. If the original cohorts are
% ambiguous, we can use the opt-readings to disambiguate. 
The original cohort may be ambiguous or not before adding the hidden readings:
in both cases, a mismatch between the remaining opt-reading and the (possibly singleton)
list of original readings will be a cause to reject.
For instance, \{\t{det}, \t{opt\_det}\} accepts the original reading \t{det};
\{\t{det}, \t{adj}, \t{opt\_det}\} accepts and disambiguates the cohort into
\t{det}; finally, \{\t{adj}, \t{opt\_det}\} rejects the whole string as not 
part of the regular language.

If there are multiple strings of the same length in the language, we have to
relax our criteria: if every cohort with a reading \t{POS} has a corresponding
\t{opt\_POS} in the set of remaining readings, such as \{\t{det}, \t{opt\_det},
\t{opt\_adj}\}, we accept the string. 

% Inari:
%   Probably skip this, we're already talking about limitations elsewhere? If
%   the original cohort is ambiguous and the opt-readings do not help to
%   disambiguate, as would be the case in \{\t{det}, \t{adj}, \t{opt\_det},
%   \t{opt\_adj}\}, we cannot disambiguate further. As we point out in
%   Section~\ref{sec:gencg}, with the example language $\{ab,ba\}$, this could
%   lead in accepting too much. 


\subsection{Limitations}
As \newcite{lager_nivre01} point out, CG has no way of expressing disjunction.
Unlike its close cousin FSIG \cite{koskenniemi90}, which would represent a 
language such as $\{ab,ba\}$ faithfully, CG substitutes uncertainty on the 
sentence level (``either $ab$ or $ba$'') with uncertainty in the cohorts: 
``the first character may be either $a$ or $b$, and the second character 
may be either $a$ or $b$''.
Given that this is a fundamental design of CG, we do not envision a way to work
around this limitation. Thus, when we use a CG generated from a regular
expression to \emph{disambiguate}, it will be overly permissive.


\section{Turing Machines in CG?}\label{sec:turingcomplete}
In the previous sections, we have assumed that CG refers to the subset of VISL
CG-3 which uses only the \t{REMOVE} command. In this section, we will take CG to
refer to the subset of VISL CG-3 which uses only the \t{ADDCOHORT} and
\t{REMCOHORT} commands, and show that this subset is Turing complete.
We will do this by implementing a procedure which translates arbitrary Turing
machines to CG, taking VISL CG-3 itself as sufficient evidence of the fact that
Turing machines can simulate constraint grammars.

\subsection{A sample Turing machine}
We will discuss our translation by means of an example Turing machine. Before we
delve into this, however, we will briefly remind the reader of the definition of
a Turing machine. A Turing machine is a 7-tuple
\[
  M = \langle Q, \Gamma, b, \Sigma, \delta, q_0, F \rangle.
\]
$Q$ is a finite, non-empty set of states, with a designated starting state $q_0
\in Q$, and a subset $F \subseteq Q$ of accepting states. $\Gamma$ is a set of
tape symbols, with a designated blank symbol $b$ and a subset $\Sigma \subseteq
\Gamma \setminus \{b\}$ of input symbols. Lastly, $\delta$ is a transition
function of the type
\[
  (Q \setminus F) \times \Gamma \to Q \times \Gamma \times \{\text{Left},\text{Right}\}.
\]
For the remainder of this section, we will use the Turing machine which computes
the successors of binary numbers as an example. This machine is given as follows:
\[\!
\begin{aligned}
  &Q&      \!\!\!\!=\;& \{\t{S0}, \t{S1}, \t{S2}, \text{Halt}\} &&\Sigma& \!\!\!\!=\;& \{\t{0}, \t{1}\} \\
  &\Gamma& \!\!\!\!=\;& \{\t{\_}, \t{0}, \t{1}\}                &&q_0&    \!\!\!\!=\;& \t{S0} \\
  &b&      \!\!\!\!=\;& \t{\_}                                  &&F&      \!\!\!\!=\;& \{\text{Halt}\}
\end{aligned}
\]
The transition function $\delta$ is described in table~\ref{tab:tm}.
What do these various states do? \t{S0} and \t{S2} both move the head of the
Turing machine to the start of the number. This leaves \t{S1} for the actual
computation. While in state \t{S1}, the head will move rightwards, overwriting
any \t{1} it encounters with a \t{0}, until it reaches either a \t{0} or the end
of the number. It then overwrites this final symbol with a \t{1}.
Table~\ref{tab:tmtrace} shows the execution trace of our sample Turing machine
for the input \t{1101}, writing the current state \emph{before} the current
position of the head.

\begin{table*}[h]
  \centering
  \begin{tabular}{cl|llc}
    \textbf{State In} & \textbf{Symbol In} & \textbf{Symbol Out} & \textbf{State Out} & \textbf{Move} \\ \hline
                   & Read \t{"\_"} & Write \t{"\_"} & \t{"S1"} & Right \\
    \t{"S0"}       & Read \t{"0"}  & Write \t{"0"}  & \t{"S0"} & Left  \\
                   & Read \t{"1"}  & Write \t{"1"}  & \t{"S0"} & Left  \\ \cline{1-5}
                   & Read \t{"\_"} & Write \t{"1"}  & \t{"S2"} & Left  \\
    \t{"S1"}       & Read \t{"0"}  & Write \t{"1"}  & \t{"S2"} & Left  \\
                   & Read \t{"1"}  & Write \t{"0"}  & \t{"S1"} & Right \\ \cline{1-5}
                   & Read \t{"\_"} & Write \t{"\_"} & Halt     & Right \\ 
    \t{"S2"}       & Read \t{"0"}  & Write \t{"0"}  & \t{"S2"} & Left  \\
                   & Read \t{"1"}  & Write \t{"1"}  & \t{"S2"} & Left  
  \end{tabular}
  \caption{Sample Turing machine (binary successor function)}
  \label{tab:tm}
\end{table*}

\begin{table*}[h]
  \centering
  \begin{tabular}{ccccccccccc}
    \t{"<c>"}&\h{"<s>"}&\t{"<c>"}&\h{"<s>"}&\t{"<c>"}&\h{"<s>"}&\t{"<c>"}&\h{"<s>"}&\t{"<c>"}&\h{"<s>"}&\t{"<c>"}\\
    \t{"\_"}&        &\t{"\_"}&\h{"S0"}&\t{"1"}&        &\t{"1"}&        &\t{"0"}&                   &\t{"1"}\\
    \t{"\_"}&\h{"S0"}&\t{"\_"}&        &\t{"1"}&        &\t{"1"}&        &\t{"0"}&                   &\t{"1"}\\
    \t{"\_"}&        &\t{"\_"}&\h{"S1"}&\t{"1"}&        &\t{"1"}&        &\t{"0"}&                   &\t{"1"}\\
    \t{"\_"}&        &\t{"\_"}&        &\t{"0"}&\h{"S1"}&\t{"1"}&        &\t{"0"}&                   &\t{"1"}\\
    \t{"\_"}&        &\t{"\_"}&        &\t{"0"}&        &\t{"0"}&\h{"S1"}&\t{"0"}&\hphantom{\t{"S1"}}&\t{"1"}\\
    \t{"\_"}&        &\t{"\_"}&        &\t{"0"}&\h{"S2"}&\t{"0"}&        &\t{"1"}&                   &\t{"1"}\\
    \t{"\_"}&        &\t{"\_"}&\h{"S2"}&\t{"0"}&        &\t{"0"}&        &\t{"1"}&                   &\t{"1"}\\
    \t{"\_"}&\h{"S2"}&\t{"\_"}&        &\t{"0"}&        &\t{"0"}&        &\t{"1"}&                   &\t{"1"}\\
    \t{"\_"}&        &\t{"\_"}&\h{"S2"}&\t{"0"}&        &\t{"0"}&        &\t{"1"}&                   &\t{"1"}
  \end{tabular}
  \caption{Execution trace of a Turing machine (see table~\ref{tab:tm}) for input \t{1101}}
  \label{tab:tmtrace}
\end{table*}

\subsection{Representing the tape}

\begin{center}
  \renewcommand{\tabcolsep}{2.5pt}
  \begin{tabular}{ccccc}
    \t{"<c>"} & \t{"<c>"} & \t{"<c>"} & \t{"<c>"} \\
    \t{"1"}   & \t{"1"}   & \t{"0"}   & \t{"1"}  
  \end{tabular}
\end{center}

\begin{center}
  \renewcommand{\tabcolsep}{2.5pt}
  \begin{tabular}{ccccc}
    \t{"<c>"} & \t{"<c>"} & \h{"<s>"} & \t{"<c>"} & \t{"<c>"} \\
    \t{"0"}   & \t{"1"}   & \h{"S1"}  & \t{"0"}   & \t{"1"}
  \end{tabular}
\end{center}

\begin{Verbatim}
BEFORE-SECTIONS
ADDCOHORT ("<S>" "S0") 
    BEFORE ("<C>") IF (-1 (>>>));
\end{Verbatim}

\begin{Verbatim}
ADD ("<S>" "OLD") ("<S>");
ADD ("<C>" "OLD") ("<C>")
    IF (-1 ("<S>" "OLD"));
\end{Verbatim}

\begin{Verbatim}
ADDCOHORT ("<S>" "S1")
    BEFORE ("<C>")
    IF (-2 ("<S>" "S1" "OLD") 
    LINK 1 ("<C>" "1" "OLD"));
ADDCOHORT ("<C>" "0")
    AFTER ("<C>" "1" "OLD")
    IF (-1 ("<S>" "S1" "OLD"));
\end{Verbatim}

\begin{Verbatim}
ADDCOHORT ("<C>" "_")
    BEFORE ("<S>")
    IF (-1 (>>>));
ADDCOHORT ("<C>" "_") 
    AFTER ("<C>")
    IF (0 (<<<) LINK -1 ("<S>"));
\end{Verbatim}

\begin{Verbatim}
REMCOHORT ("<S>" "OLD");
REMCOHORT ("<C>" "OLD");
\end{Verbatim}

\begin{Verbatim}
AFTER-SECTIONS
REMCOHORT ("<C>" "_") IF (NOT -1* SYM);
REMCOHORT ("<C>" "_") IF (NOT  1* SYM);
\end{Verbatim}

\section{Discussion}
At the time of writing, the grammars generated by the FSA$\rightarrow$CG
conversion tool look awkward, and involve a number of extra cohorts and symbols.
However, it does give us the ability to quickly generate fragments of CG code
which disambiguate or rewrite input based on a regular expression.
We are hoping to develop this further, to also include context-free or even
mildly context-sensitive grammars.
Such CGs could be used as, e.g.\ as part of a larger constraint grammar.
In addition, we would like to focus on making the grammars more human-readable,
so that they could be used to quickly generate a basis for a constraint grammar
from existing context-free grammars (or equivalent formalisms).
This could serve as an alternative to learning grammars from a corpus.


\section{Related Work}
\newcite{tapanainen1999phd} gives an account of the expressivity of
the contextual tests for 4 different constraint formalisms, including CG. 
In addition, parsing complexity can be easily defined for a given variant and 
implementation of CG; see for instance \newcite{nemeskey14}.
\newcite{ylijyra2017} presents questions regarding the expressive power
of Constraint Grammar, concentrating on the implementation side.
To our knowledge, CG as a generative model has not been been approached before.

\bibliographystyle{acl}
\bibliography{cg}

\end{document}
